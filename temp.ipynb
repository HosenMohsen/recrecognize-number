{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230fac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd33565",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([2.0], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# Input\n",
    "x = torch.tensor([3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57397e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: 7.0\n"
     ]
    }
   ],
   "source": [
    "#Forward pass\n",
    "y_pred = w * x + b\n",
    "print(f\"y_pred: {y_pred.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e027708f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.0\n"
     ]
    }
   ],
   "source": [
    "#Loss\n",
    "y_true = torch.tensor([10.0])\n",
    "loss = (y_pred - y_true) ** 2\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19358652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant backward: w.grad = None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Avant backward: w.grad = {w.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1def6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aae0b2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après backward: w.grad = tensor([-18.])\n",
      "Après backward: b.grad = tensor([-6.])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Après backward: w.grad = {w.grad}\")\n",
    "print(f\"Après backward: b.grad = {b.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b9d46ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update manuel des poids\n",
    "learning_rate = 0.01\n",
    "with torch.no_grad():  #Désactiver le tracking des gradients pour l'update w = w + alpha * w\n",
    "    w -= learning_rate * w.grad\n",
    "    b -= learning_rate * b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e876cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouveaux poids: w = tensor([2.1800], requires_grad=True), b = tensor([1.0600], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nouveaux poids: w = {w}, b = {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e12f610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après zero_: w.grad = tensor([0.]), b.grad = tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "#Réinitialiser les gradients (IMPORTANT !)\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(f\"Après zero_: w.grad = {w.grad}, b.grad = {b.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e67a08e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poids: Parameter containing:\n",
      "tensor([[0.5113]], requires_grad=True)\n",
      "Biais: Parameter containing:\n",
      "tensor([-0.5247], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "perceptron = nn.Linear(1, 1)\n",
    "\n",
    "#Voir les poids initiaux\n",
    "print(f\"Poids: {perceptron.weight}\")\n",
    "print(f\"Biais: {perceptron.bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5931861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[3.]])\n",
      "tensor([[1.0093]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "x = torch.tensor([[3.0]]) # Shape (batch_size, features)\n",
    "print(f\"input: {x}\")\n",
    "y_pred = perceptron(x)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "373f3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "y_true = torch.tensor([[10.0]])\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98ea0914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(80.8328, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c574453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cca880b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient du poids: tensor([[-53.9443]])\n",
      "Gradient du biais: tensor([-17.9814])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient du poids: {perceptron.weight.grad}\")\n",
    "print(f\"Gradient du biais: {perceptron.bias.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ace6046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(perceptron.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "751ca94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70f93db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouveaux poids: Parameter containing:\n",
      "tensor([[1.0508]], requires_grad=True) Parameter containing:\n",
      "tensor([-0.3449], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nouveaux poids: {perceptron.weight} {perceptron.bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef4d567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6915853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient du poids: None\n",
      "Gradient du biais: None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient du poids: {perceptron.weight.grad}\")\n",
    "print(f\"Gradient du biais: {perceptron.bias.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49542bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module) : \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 4)\n",
    "        self.fc2 = nn.Linear(4, 3)\n",
    "        self.fc3 = nn.Linear(3, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b18172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cdf0ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNet(\n",
       "  (fc1): Linear(in_features=2, out_features=4, bias=True)\n",
       "  (fc2): Linear(in_features=4, out_features=3, bias=True)\n",
       "  (fc3): Linear(in_features=3, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "300a35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0])\n",
    "y = torch.tensor([5.0])\n",
    "y_pred = model(x)\n",
    "loss = nn.MSELoss()(y_pred, y)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19a6b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "464a149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fc478c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(training_data, batch_size=64, shuffle=True, pin_memory=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c62b9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "122a8224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
      "torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(training_data.classes)\n",
    "print(training_data.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99ce458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e62cf6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0.dev20241112+cu121\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f93b1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8ec27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b958215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1442, -0.0351,  0.0640,  0.0011,  0.1327,  0.0521,  0.0944, -0.0304,\n",
       "          0.0039, -0.1137]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e043bbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1116, 0.0933, 0.1030, 0.0967, 0.1103, 0.1018, 0.1062, 0.0937, 0.0970,\n",
       "         0.0862]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "pred_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6baa7deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pred_probab.argmax(1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b41bc7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94cfeb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch_idx, batch_value in enumerate(dataloader):\n",
    "        X, y = batch_value\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            loss, current = loss.item(), (batch_idx+1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd57391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len (dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss/=num_batches\n",
    "    correct/=size\n",
    "    print(f\"Test Error: \\n Accuaracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "147facae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------\n",
      "loss: 0.173667  [   64/60000]\n",
      "loss: 0.056677  [ 6464/60000]\n",
      "loss: 0.097287  [12864/60000]\n",
      "loss: 0.358566  [19264/60000]\n",
      "loss: 0.184349  [25664/60000]\n",
      "loss: 0.185151  [32064/60000]\n",
      "loss: 0.072494  [38464/60000]\n",
      "loss: 0.134824  [44864/60000]\n",
      "loss: 0.116970  [51264/60000]\n",
      "loss: 0.042755  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 95.7%, Avg loss: 0.140216 \n",
      "\n",
      "Epoch 2\n",
      "---------------------\n",
      "loss: 0.113894  [   64/60000]\n",
      "loss: 0.137595  [ 6464/60000]\n",
      "loss: 0.084001  [12864/60000]\n",
      "loss: 0.216962  [19264/60000]\n",
      "loss: 0.129510  [25664/60000]\n",
      "loss: 0.074802  [32064/60000]\n",
      "loss: 0.179214  [38464/60000]\n",
      "loss: 0.107252  [44864/60000]\n",
      "loss: 0.143951  [51264/60000]\n",
      "loss: 0.183223  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 95.4%, Avg loss: 0.148490 \n",
      "\n",
      "Epoch 3\n",
      "---------------------\n",
      "loss: 0.092988  [   64/60000]\n",
      "loss: 0.198373  [ 6464/60000]\n",
      "loss: 0.199868  [12864/60000]\n",
      "loss: 0.221326  [19264/60000]\n",
      "loss: 0.120574  [25664/60000]\n",
      "loss: 0.078816  [32064/60000]\n",
      "loss: 0.157214  [38464/60000]\n",
      "loss: 0.110833  [44864/60000]\n",
      "loss: 0.132411  [51264/60000]\n",
      "loss: 0.146239  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 95.4%, Avg loss: 0.142095 \n",
      "\n",
      "Epoch 4\n",
      "---------------------\n",
      "loss: 0.043598  [   64/60000]\n",
      "loss: 0.077128  [ 6464/60000]\n",
      "loss: 0.078390  [12864/60000]\n",
      "loss: 0.120667  [19264/60000]\n",
      "loss: 0.110643  [25664/60000]\n",
      "loss: 0.158916  [32064/60000]\n",
      "loss: 0.082959  [38464/60000]\n",
      "loss: 0.110012  [44864/60000]\n",
      "loss: 0.190931  [51264/60000]\n",
      "loss: 0.036826  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 95.9%, Avg loss: 0.134094 \n",
      "\n",
      "Epoch 5\n",
      "---------------------\n",
      "loss: 0.207442  [   64/60000]\n",
      "loss: 0.164569  [ 6464/60000]\n",
      "loss: 0.059251  [12864/60000]\n",
      "loss: 0.155612  [19264/60000]\n",
      "loss: 0.183444  [25664/60000]\n",
      "loss: 0.163537  [32064/60000]\n",
      "loss: 0.220088  [38464/60000]\n",
      "loss: 0.140122  [44864/60000]\n",
      "loss: 0.118685  [51264/60000]\n",
      "loss: 0.161141  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 96.3%, Avg loss: 0.124588 \n",
      "\n",
      "Epoch 6\n",
      "---------------------\n",
      "loss: 0.157457  [   64/60000]\n",
      "loss: 0.128655  [ 6464/60000]\n",
      "loss: 0.069381  [12864/60000]\n",
      "loss: 0.259490  [19264/60000]\n",
      "loss: 0.086054  [25664/60000]\n",
      "loss: 0.126125  [32064/60000]\n",
      "loss: 0.081103  [38464/60000]\n",
      "loss: 0.056434  [44864/60000]\n",
      "loss: 0.055471  [51264/60000]\n",
      "loss: 0.156432  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 96.5%, Avg loss: 0.117001 \n",
      "\n",
      "Epoch 7\n",
      "---------------------\n",
      "loss: 0.183916  [   64/60000]\n",
      "loss: 0.117767  [ 6464/60000]\n",
      "loss: 0.110461  [12864/60000]\n",
      "loss: 0.163736  [19264/60000]\n",
      "loss: 0.055055  [25664/60000]\n",
      "loss: 0.075987  [32064/60000]\n",
      "loss: 0.066727  [38464/60000]\n",
      "loss: 0.101912  [44864/60000]\n",
      "loss: 0.198383  [51264/60000]\n",
      "loss: 0.086042  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 96.2%, Avg loss: 0.118450 \n",
      "\n",
      "Epoch 8\n",
      "---------------------\n",
      "loss: 0.053794  [   64/60000]\n",
      "loss: 0.115507  [ 6464/60000]\n",
      "loss: 0.089224  [12864/60000]\n",
      "loss: 0.090877  [19264/60000]\n",
      "loss: 0.121425  [25664/60000]\n",
      "loss: 0.101653  [32064/60000]\n",
      "loss: 0.243673  [38464/60000]\n",
      "loss: 0.170086  [44864/60000]\n",
      "loss: 0.126920  [51264/60000]\n",
      "loss: 0.088783  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 96.5%, Avg loss: 0.116994 \n",
      "\n",
      "Epoch 9\n",
      "---------------------\n",
      "loss: 0.074704  [   64/60000]\n",
      "loss: 0.133614  [ 6464/60000]\n",
      "loss: 0.096316  [12864/60000]\n",
      "loss: 0.050945  [19264/60000]\n",
      "loss: 0.137986  [25664/60000]\n",
      "loss: 0.049124  [32064/60000]\n",
      "loss: 0.084911  [38464/60000]\n",
      "loss: 0.073173  [44864/60000]\n",
      "loss: 0.060454  [51264/60000]\n",
      "loss: 0.027489  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 96.3%, Avg loss: 0.118535 \n",
      "\n",
      "Epoch 10\n",
      "---------------------\n",
      "loss: 0.040829  [   64/60000]\n",
      "loss: 0.041456  [ 6464/60000]\n",
      "loss: 0.034616  [12864/60000]\n",
      "loss: 0.050425  [19264/60000]\n",
      "loss: 0.212374  [25664/60000]\n",
      "loss: 0.081622  [32064/60000]\n",
      "loss: 0.091943  [38464/60000]\n",
      "loss: 0.050020  [44864/60000]\n",
      "loss: 0.031440  [51264/60000]\n",
      "loss: 0.189771  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 96.4%, Avg loss: 0.116080 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Assurez-vous que le modèle est bien sur le GPU avant de commencer\n",
    "model.to(device)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n---------------------\")\n",
    "    train(training_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f05bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m example_inputs = (torch)\n\u001b[32m      2\u001b[39m model.to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m onnx_program = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43monnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m onnx_program.save(\u001b[33m\"\u001b[39m\u001b[33mmodel.onnx\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m0m0x\\OneDrive\\Bureau\\A4-CDI\\ia\\.venv\\Lib\\site-packages\\torch\\onnx\\__init__.py:350\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(model, args, f, kwargs, export_params, verbose, input_names, output_names, opset_version, dynamic_axes, keep_initializers_as_inputs, dynamo, external_data, dynamic_shapes, custom_translation_table, report, optimize, verify, profile, dump_exported_program, artifacts_dir, fallback, training, operator_export_type, do_constant_folding, custom_opsets, export_modules_as_functions, autograd_inlining, **_)\u001b[39m\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, torch.Tensor):\n\u001b[32m    349\u001b[39m         args = (args,)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport_compat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_translation_table\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_translation_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexternal_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexternal_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdump_exported_program\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdump_exported_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43martifacts_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43martifacts_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01monnx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m0m0x\\OneDrive\\Bureau\\A4-CDI\\ia\\.venv\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_compat.py:154\u001b[39m, in \u001b[36mexport_compat\u001b[39m\u001b[34m(model, args, f, kwargs, export_params, verbose, input_names, output_names, opset_version, custom_translation_table, dynamic_axes, dynamic_shapes, keep_initializers_as_inputs, external_data, report, optimize, verify, profile, dump_exported_program, artifacts_dir, fallback, **_)\u001b[39m\n\u001b[32m    152\u001b[39m     dynamic_shapes = dynamic_shapes \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     args, kwargs = \u001b[43m_get_torch_export_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dynamic_shapes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dynamic_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    156\u001b[39m         dynamic_shapes = _from_dynamic_axes_to_dynamic_shapes(\n\u001b[32m    157\u001b[39m             model,\n\u001b[32m    158\u001b[39m             dynamic_axes=dynamic_axes,\n\u001b[32m    159\u001b[39m             input_names=input_names,\n\u001b[32m    160\u001b[39m             output_names=\u001b[38;5;28mset\u001b[39m(output_names \u001b[38;5;129;01mor\u001b[39;00m ()),\n\u001b[32m    161\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m0m0x\\OneDrive\\Bureau\\A4-CDI\\ia\\.venv\\Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_compat.py:108\u001b[39m, in \u001b[36m_get_torch_export_args\u001b[39m\u001b[34m(args, kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_torch_export_args\u001b[39m(\n\u001b[32m    104\u001b[39m     args: \u001b[38;5;28mtuple\u001b[39m[Any, ...],\n\u001b[32m    105\u001b[39m     kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    106\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[Any, ...], \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[32m    107\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Obtain the arguments for torch.onnx.export from the model and the input arguments.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    109\u001b[39m         kwargs = args[-\u001b[32m1\u001b[39m]\n\u001b[32m    110\u001b[39m         args = args[:-\u001b[32m1\u001b[39m]\n",
      "\u001b[31mTypeError\u001b[39m: 'module' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#cela n'a pas marché du coup j'ai fait autre chose en bas \n",
    "\n",
    "example_inputs = (torch)\n",
    "model.to(\"cpu\")\n",
    "onnx_program = torch.onnx.export(model, example_inputs, dynamo=True)\n",
    "onnx_program.save(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c12baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `NeuralNetwork([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `NeuralNetwork([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "✅ Modèle exporté : model.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 28, 28)\n",
    "\n",
    "export_output = torch.onnx.export(\n",
    "    model, \n",
    "    (dummy_input,), \n",
    "    dynamo=True\n",
    ")\n",
    " \n",
    "export_output.save(\"model.onnx\")\n",
    "print(\"Modèle exporté : model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe09e400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
