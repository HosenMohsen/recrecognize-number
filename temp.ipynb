{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "230fac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ffd33565",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([2.0], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# Input\n",
    "x = torch.tensor([3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "57397e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: 7.0\n"
     ]
    }
   ],
   "source": [
    "#Forward pass\n",
    "y_pred = w * x + b\n",
    "print(f\"y_pred: {y_pred.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e027708f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.0\n"
     ]
    }
   ],
   "source": [
    "#Loss\n",
    "y_true = torch.tensor([10.0])\n",
    "loss = (y_pred - y_true) ** 2\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "19358652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant backward: w.grad = None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Avant backward: w.grad = {w.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1def6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "aae0b2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après backward: w.grad = tensor([-18.])\n",
      "Après backward: b.grad = tensor([-6.])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Après backward: w.grad = {w.grad}\")\n",
    "print(f\"Après backward: b.grad = {b.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6b9d46ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update manuel des poids\n",
    "learning_rate = 0.01\n",
    "with torch.no_grad():  #Désactiver le tracking des gradients pour l'update w = w + alpha * w\n",
    "    w -= learning_rate * w.grad\n",
    "    b -= learning_rate * b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6e876cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouveaux poids: w = tensor([2.1800], requires_grad=True), b = tensor([1.0600], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nouveaux poids: w = {w}, b = {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8e12f610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après zero_: w.grad = tensor([0.]), b.grad = tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "#Réinitialiser les gradients (IMPORTANT !)\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(f\"Après zero_: w.grad = {w.grad}, b.grad = {b.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e67a08e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poids: Parameter containing:\n",
      "tensor([[-0.6666]], requires_grad=True)\n",
      "Biais: Parameter containing:\n",
      "tensor([0.2563], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "perceptron = nn.Linear(1, 1)\n",
    "\n",
    "#Voir les poids initiaux\n",
    "print(f\"Poids: {perceptron.weight}\")\n",
    "print(f\"Biais: {perceptron.bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b5931861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[3.]])\n",
      "tensor([[-1.7434]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "x = torch.tensor([[3.0]]) # Shape (batch_size, features)\n",
    "print(f\"input: {x}\")\n",
    "y_pred = perceptron(x)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "373f3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "y_true = torch.tensor([[10.0]])\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "98ea0914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(137.9067, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9c574453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "cca880b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient du poids: tensor([[-70.4602]])\n",
      "Gradient du biais: tensor([-23.4867])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient du poids: {perceptron.weight.grad}\")\n",
    "print(f\"Gradient du biais: {perceptron.bias.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8ace6046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(perceptron.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "751ca94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "70f93db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouveaux poids: Parameter containing:\n",
      "tensor([[0.0381]], requires_grad=True) Parameter containing:\n",
      "tensor([0.4912], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nouveaux poids: {perceptron.weight} {perceptron.bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ef4d567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6915853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient du poids: None\n",
      "Gradient du biais: None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient du poids: {perceptron.weight.grad}\")\n",
    "print(f\"Gradient du biais: {perceptron.bias.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "49542bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module) : \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, 16 * 7 * 7)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3b18172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9cdf0ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNet(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=784, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "300a35fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m0m0x\\OneDrive\\Bureau\\A4-CDI\\ia\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 28, 28)\n",
    "y = torch.tensor([5.0])\n",
    "y_pred = model(x)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = nn.MSELoss()(y_pred, y)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "19a6b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5fc478c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(training_data, batch_size=64, shuffle=True, pin_memory=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7c62b9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "122a8224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
      "torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(training_data.classes)\n",
    "print(training_data.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "99ce458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.BatchNorm1d(512),        \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),       \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e62cf6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0.dev20241112+cu121\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8f93b1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a8ec27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9b958215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0553,  0.0187, -0.0342, -0.0067, -0.0616, -0.0608,  0.0860, -0.0399,\n",
       "         -0.0912, -0.0385]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e043bbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1074, 0.1035, 0.0982, 0.1009, 0.0955, 0.0956, 0.1107, 0.0976, 0.0927,\n",
       "         0.0978]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "pred_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "6baa7deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6], device='cuda:0')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pred_probab.argmax(1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b41bc7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "00b31de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "94cfeb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, epoch_index, writer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch_idx, batch_value in enumerate(dataloader):\n",
    "        X, y = batch_value\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        global_step = epoch_index * len(dataloader) + batch_idx\n",
    "        writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            loss, current = loss.item(), (batch_idx+1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "dd57391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, epoch_index, writer):\n",
    "    size = len (dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss/=num_batches\n",
    "    correct/=size\n",
    "\n",
    "    writer.add_scalar('Accuracy/test', correct, epoch_index)\n",
    "    writer.add_scalar('Loss/test', test_loss, epoch_index)\n",
    "    \n",
    "    print(f\"Test Error: \\n Accuaracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "147facae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------\n",
      "loss: 2.442024  [   64/60000]\n",
      "loss: 0.260487  [ 6464/60000]\n",
      "loss: 0.209515  [12864/60000]\n",
      "loss: 0.160914  [19264/60000]\n",
      "loss: 0.140762  [25664/60000]\n",
      "loss: 0.160792  [32064/60000]\n",
      "loss: 0.163237  [38464/60000]\n",
      "loss: 0.301405  [44864/60000]\n",
      "loss: 0.113815  [51264/60000]\n",
      "loss: 0.198686  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 96.9%, Avg loss: 0.097881 \n",
      "\n",
      "Epoch 2\n",
      "---------------------\n",
      "loss: 0.133526  [   64/60000]\n",
      "loss: 0.091598  [ 6464/60000]\n",
      "loss: 0.119151  [12864/60000]\n",
      "loss: 0.112127  [19264/60000]\n",
      "loss: 0.056525  [25664/60000]\n",
      "loss: 0.183318  [32064/60000]\n",
      "loss: 0.066220  [38464/60000]\n",
      "loss: 0.116279  [44864/60000]\n",
      "loss: 0.174885  [51264/60000]\n",
      "loss: 0.032398  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 97.5%, Avg loss: 0.076834 \n",
      "\n",
      "Epoch 3\n",
      "---------------------\n",
      "loss: 0.094847  [   64/60000]\n",
      "loss: 0.188158  [ 6464/60000]\n",
      "loss: 0.144115  [12864/60000]\n",
      "loss: 0.095431  [19264/60000]\n",
      "loss: 0.094196  [25664/60000]\n",
      "loss: 0.099909  [32064/60000]\n",
      "loss: 0.052367  [38464/60000]\n",
      "loss: 0.075186  [44864/60000]\n",
      "loss: 0.080969  [51264/60000]\n",
      "loss: 0.172087  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 97.8%, Avg loss: 0.071859 \n",
      "\n",
      "Epoch 4\n",
      "---------------------\n",
      "loss: 0.126897  [   64/60000]\n",
      "loss: 0.026042  [ 6464/60000]\n",
      "loss: 0.056834  [12864/60000]\n",
      "loss: 0.073333  [19264/60000]\n",
      "loss: 0.050237  [25664/60000]\n",
      "loss: 0.075991  [32064/60000]\n",
      "loss: 0.010067  [38464/60000]\n",
      "loss: 0.027207  [44864/60000]\n",
      "loss: 0.079455  [51264/60000]\n",
      "loss: 0.044511  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 97.9%, Avg loss: 0.063267 \n",
      "\n",
      "Epoch 5\n",
      "---------------------\n",
      "loss: 0.093564  [   64/60000]\n",
      "loss: 0.044701  [ 6464/60000]\n",
      "loss: 0.084110  [12864/60000]\n",
      "loss: 0.030604  [19264/60000]\n",
      "loss: 0.059074  [25664/60000]\n",
      "loss: 0.040758  [32064/60000]\n",
      "loss: 0.013530  [38464/60000]\n",
      "loss: 0.026190  [44864/60000]\n",
      "loss: 0.066703  [51264/60000]\n",
      "loss: 0.022579  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 97.9%, Avg loss: 0.066699 \n",
      "\n",
      "Epoch 6\n",
      "---------------------\n",
      "loss: 0.070572  [   64/60000]\n",
      "loss: 0.011769  [ 6464/60000]\n",
      "loss: 0.005562  [12864/60000]\n",
      "loss: 0.132128  [19264/60000]\n",
      "loss: 0.017012  [25664/60000]\n",
      "loss: 0.030075  [32064/60000]\n",
      "loss: 0.120856  [38464/60000]\n",
      "loss: 0.027879  [44864/60000]\n",
      "loss: 0.043312  [51264/60000]\n",
      "loss: 0.055531  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 98.2%, Avg loss: 0.056146 \n",
      "\n",
      "Epoch 7\n",
      "---------------------\n",
      "loss: 0.049624  [   64/60000]\n",
      "loss: 0.045084  [ 6464/60000]\n",
      "loss: 0.022387  [12864/60000]\n",
      "loss: 0.060998  [19264/60000]\n",
      "loss: 0.006838  [25664/60000]\n",
      "loss: 0.153527  [32064/60000]\n",
      "loss: 0.034236  [38464/60000]\n",
      "loss: 0.011183  [44864/60000]\n",
      "loss: 0.026281  [51264/60000]\n",
      "loss: 0.005105  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 98.2%, Avg loss: 0.060209 \n",
      "\n",
      "Epoch 8\n",
      "---------------------\n",
      "loss: 0.025488  [   64/60000]\n",
      "loss: 0.011007  [ 6464/60000]\n",
      "loss: 0.014434  [12864/60000]\n",
      "loss: 0.011043  [19264/60000]\n",
      "loss: 0.012358  [25664/60000]\n",
      "loss: 0.030686  [32064/60000]\n",
      "loss: 0.095926  [38464/60000]\n",
      "loss: 0.009135  [44864/60000]\n",
      "loss: 0.063979  [51264/60000]\n",
      "loss: 0.036684  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 98.2%, Avg loss: 0.054541 \n",
      "\n",
      "Epoch 9\n",
      "---------------------\n",
      "loss: 0.007804  [   64/60000]\n",
      "loss: 0.029540  [ 6464/60000]\n",
      "loss: 0.037246  [12864/60000]\n",
      "loss: 0.006337  [19264/60000]\n",
      "loss: 0.043459  [25664/60000]\n",
      "loss: 0.084971  [32064/60000]\n",
      "loss: 0.015059  [38464/60000]\n",
      "loss: 0.013128  [44864/60000]\n",
      "loss: 0.005975  [51264/60000]\n",
      "loss: 0.011298  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 98.2%, Avg loss: 0.054928 \n",
      "\n",
      "Epoch 10\n",
      "---------------------\n",
      "loss: 0.007790  [   64/60000]\n",
      "loss: 0.019649  [ 6464/60000]\n",
      "loss: 0.006012  [12864/60000]\n",
      "loss: 0.059060  [19264/60000]\n",
      "loss: 0.004401  [25664/60000]\n",
      "loss: 0.020354  [32064/60000]\n",
      "loss: 0.025716  [38464/60000]\n",
      "loss: 0.072642  [44864/60000]\n",
      "loss: 0.073251  [51264/60000]\n",
      "loss: 0.079752  [57664/60000]\n",
      "Test Error: \n",
      " Accuaracy: 98.3%, Avg loss: 0.054722 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Assurez-vous que le modèle est bien sur le GPU avant de commencer\n",
    "model.to(device)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n---------------------\")\n",
    "    train(training_dataloader, model, loss_fn, optimizer, t, writer)\n",
    "    test(test_dataloader, model, loss_fn, t, writer)\n",
    "    \n",
    "writer.flush()\n",
    "writer.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c9c12baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle exporté : model.onnx\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    (dummy_input,), \n",
    "    \"model.onnx\", \n",
    "    input_names=['input'],       \n",
    "    output_names=['output'],    \n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    ")\n",
    "\n",
    "print(\"Modèle exporté : model.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
